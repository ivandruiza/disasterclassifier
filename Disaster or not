{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T00:34:50.881950Z","iopub.execute_input":"2025-02-25T00:34:50.882404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Brief description of the problem and data**\n\nWe have in our hands a set of 7613 tweets, some of them with information regarding location from where there were sent and some of them with keywords. The target or label will be 1 when tweet is about a disaster and 0 otherwise.\\\nObjective is to clasify them as disaster or not, so binary class classification.","metadata":{}},{"cell_type":"markdown","source":"# *EDA*\n**Lest go with data reading some basic exploration on this cells.**\\\nWe can see a little unbalance between real disasters and other tweets (as expected disasters are less frecuent). There are lot of missing info (on location and keywords which may be not that important but we can`t know that yet). Fortunately missing info is aparently random because class distribution is the same before and after removing those items. \n\nI will stay with both databases mainly because we will lose 1/3 of data when filtering. Objective now is to use calssifiers with both databases to see if it will make a big diference when classifing items.","metadata":{}},{"cell_type":"code","source":"#Read\ndf_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv') \ndf_train_filtered = df_train.copy()  # Duplicate the dataset\ndf_train_filtered.dropna(inplace=True)  # Remove rows with null values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Exploring Dataset \n#NN\n\nprint ('----Shape----')\nprint (df_train.shape[0])\nprint ('----Columns----')\nprint (df_train.columns)\nprint ('----Head----')\nprint(df_train.head())\nprint ('----Info----')\ndf_train.info()\nprint ('----Null values on important columns (text and target)----')\ntext = df_train['text'].isnull().sum()\ntarget = df_train['target'].isnull().sum()\nprint (text, target)\nprint ('----Details----')\nloc = df_train['location'].nunique()\nword = df_train['keyword'].nunique()\nprint (f'there are {loc} diferent places, and {word} keywords')\n\n#Distribution of classes\nplt.figure(figsize=(6,6))\nax = sns.countplot(data=df_train, x='target', palette='viridis')\n# Add percentages on bars\nfor p in ax.patches:\n    percentage = f'{100 * p.get_height() / df_train.shape[0]:.2f}%'  # Calculate %\n    ax.annotate(percentage, (p.get_x() + p.get_width() / 2, p.get_height()), \n                ha='center', va='bottom', fontsize=12, color='black')\nplt.title('Class Distribution with Percentages')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#After getting rid of rows with incomplete information\n\nprint ('----Shape----')\nprint (df_train_filtered.shape[0])\nprint ('----Info----')\ndf_train_filtered.info()\n\n#Distribution of classes\nplt.figure(figsize=(6,6))\nax = sns.countplot(data=df_train, x='target', palette='viridis')\n# Add percentages on bars\nfor p in ax.patches:\n    percentage = f'{100 * p.get_height() / df_train.shape[0]:.2f}%'  # Calculate %\n    ax.annotate(percentage, (p.get_x() + p.get_width() / 2, p.get_height()), \n                ha='center', va='bottom', fontsize=12, color='black')\nplt.title('Class Distribution with Percentages')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Look into vocabulary **\nI am goint to use tokenizer from keras for this part. \n\ncheck: https://keras.io/keras_hub/api/tokenizers/tokenizer/\n","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"#number of words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train['text'])\nword_index = tokenizer.word_index  # Dictionary of word-to-index mapping\nvocab_size = len(word_index)  # Total unique words in vocabulary\nprint(f\"Vocabulary Size: {vocab_size}\")\n\n#top used words\nword_counts = tokenizer.word_counts\nsorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n\nprint(sorted_words[:50])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Removing Stopwords**\nAfter looking a little into most used words it seem most of them are practically stop words and links, so lets remove them.\nLinks can be usefull but we need to group them and make them binary (tweet has a link or no).\nre or regular expresion docs are found here:\n\nhttps://docs.python.org/3/library/re.html#regular-expression-syntax.\n\nhttps://www.geeksforgeeks.org/removing-stop-words-nltk-python/","metadata":{}},{"cell_type":"code","source":"import re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\npersonal_words = {'t', 'co', 'http', 'https', 'the', 'a', 'u', 'rt', 'in', 'to', 'of', 'and', 'i','im', 'its', 'is', \n                  'for', 'on', 'you', 'my', 'with', 'that', 'it', 'at', 'by', 'this', 'from', 'are', \n                  'be', 'was', 'amp', 'as', 'me', 'so', 'not', 'your', 'out', 'no'}\n\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'http[s]?://\\S+', 'url', text)  # Replace links with 'URL'   \n    text = re.sub(r'[^a-z\\s]', '', text)  # Remove numbers, punctuation, and special chars\n    words = text.split()\n    words = [word for word in words if word not in personal_words]  # Remove stopwords\n\n    if not words:  # If all words are removed, keep at least the 'URL' marker\n        return 'URL' if 'URL' in text else ''  \n\n    return ' '.join(words)\n    \n# Apply cleaning function\ndf_train['clean_text'] = df_train['text'].astype(str).apply(clean_text)\n\n# Tokenize again without stopwords\ntokenizer_filtered = Tokenizer()\ntokenizer_filtered.fit_on_texts(df_train['clean_text'])\n\n# Get word frequencies\nword_counts_filtered = tokenizer_filtered.word_counts\nsorted_words_filtered = sorted(word_counts_filtered.items(), key=lambda x: x[1], reverse=True)\n\n# Print top 50 most common words\nprint(\"Top 50 most common words:\")\nfor word, count in sorted_words_filtered[:25]:\n    print(f\"{word}: {count}\")\n\nword_index_filtered = tokenizer_filtered.word_index  # Dictionary of word-to-index mapping\nvocab_size_filtered = len(word_index_filtered)  # Total unique words in vocabulary\nprint(f\"Vocabulary Size: {vocab_size_filtered}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plots and graphs** after filtering and removing stopwords and punctuation (take a look at total number of words).","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Function to generate word cloud for a specific class\ndef generate_wordcloud(target_class, color):\n    text = \" \".join(df_train[df_train['target'] == target_class]['clean_text'])\n    wordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap=color, max_words=50).generate(text)\n    \n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(f\"Word Cloud for Class {target_class} ({'Disaster' if target_class == 1 else 'Non-Disaster'})\", fontsize=14)\n    plt.show()\n\n# Generate word clouds\ngenerate_wordcloud(0, \"Blues\")  # Non-Disaster (Class 0) in blue shades\ngenerate_wordcloud(1, \"Reds\")   # Disaster (Class 1) in red shades\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Function to calculate percentage of tweets containing a specific word\ndef word_percentage(word):\n    class_0_count = df_train[df_train['target'] == 0]['clean_text'].apply(lambda x: word in x).mean() * 100\n    class_1_count = df_train[df_train['target'] == 1]['clean_text'].apply(lambda x: word in x).mean() * 100\n    return class_0_count, class_1_count\n\n# Get percentages\nurl_pct_0, url_pct_1 = word_percentage(\"url\")  # URLs\n\n# Prepare data for plotting\nwords = [\"url\"]\nclass_0_values = [url_pct_0]\nclass_1_values = [url_pct_1]\n\n# Plot bar chart\nfig, ax = plt.subplots(figsize=(8, 5))\nbar_width = 0.4\nindex = range(len(words))\n\n# Plot bars for both classes\nax.bar(index, class_0_values, bar_width, label=\"Non-Disaster (Class 0)\", color=\"blue\", alpha=0.7)\nax.bar([i + bar_width for i in index], class_1_values, bar_width, label=\"Disaster (Class 1)\", color=\"red\", alpha=0.7)\n\n# Formatting\nax.set_xticks([i + bar_width / 2 for i in index])\nax.set_xticklabels(words)\nax.set_ylabel(\"Percentage of Tweets (%)\")\nax.set_title(\"Percentage of Tweets Containing 'URL' in Each Class\")\nax.legend()\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture\n\n\nIn the next steps we are going to try different aproaches.\n\nThis is first model. Using just the text of the tweets and a simple secuential model.","metadata":{}},{"cell_type":"code","source":"#imports\nimport tensorflow as tf\nimport keras_tuner as kt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras.models import Model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model 1**\n\nThis model is:\n1. Converting words into tokens\n2. Converting sentences into secuence of tokens\n3. Padding those secences to the longest secuence - 8\n4. Secuential model as follows\n* 4.1. Embeding layer: Maps 10000 most used words to a 128 vector for understanding words relationships\n* 4.2 LSTM layer with Regularization\n* 4.3 Dense layer as a classifier (disaster or not tweet)\n\n\"**Embedding**\nThe embedding layer represents data, such as words or categories, in a more meaningful form by converting them into numerical vectors that a machine can understand. It is commonly used in Natural Language Processing (NLP) and recommendation systems to handle categorical data. Since computers can only process numbers, an embedding layer helps convert large sets of data into smaller, more efficient vectors, making it easier for the machine to learn patterns.\n\nThe main uses of embedding layers include:\n\nReduce Dimensionality: It compresses high-dimensional data into a more manageable size.\nCapture Relationships: It enables the model to understand relationships between different inputs, such as words in a sentence.\nImprove Efficiency: By using dense vectors, the model processes data faster and more effectively.\"*\n\n* Taken directly from: https://www.geeksforgeeks.org/what-is-embedding-layer/\n\n","metadata":{}},{"cell_type":"code","source":"# Model 1 - Text on full DB -with filtered text\n\"\"\"\n------\nData preparation \n------\n\"\"\"\n# Set VOCAB_SIZE dynamically based on analysis\ntokenizer1 = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\ntokenizer1.fit_on_texts(df_train['text'])\n# Convert tweets to sequences\nsequences1 = tokenizer1.texts_to_sequences(df_train['clean_text'])\n# Padding\nMAX_LENGTH = (max(len(seq) for seq in sequences1)) - 8\npadded_sequences1 = pad_sequences(sequences1, maxlen=MAX_LENGTH, padding='post', truncating='post')\n\n# Labels\nlabels = np.array(df_train['target'])\nX_train, X_val, y_train, y_val = train_test_split(padded_sequences1, labels, test_size=0.2, random_state=42)\n\n# Checks \nprint(df_train['clean_text'].iloc[1])  # Original text\nprint(sequences1[1])  # Tokenized version\nprint(padded_sequences1[0])  # Padded tokenized tweet\n\n\"\"\"\n------\nModel Build and Train\n------\n\"\"\"\n\nmodel = Sequential([\n    Embedding(input_dim=10000, output_dim=128, input_length=MAX_LENGTH),\n    LSTM(64, return_sequences=False),  # LSTM layer with Regularization\n    Dense(1, activation='sigmoid')  # Output layer (binary classification)\n])\n\n# Compile \noptimizer = Adam(learning_rate=0.00025)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n# Train the model\nhistory1 = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=10,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model 2: Hyperparameter Tuning**\n\nModel is acceptable, but lets play with parameters. \nIn this section the objective is to move around embedding dimensions, LSTM units, use L2 regularization for penalty, use Dopout with various values, change adam learning rate and iterate over the model to get best parameters.\nFor the first tunning, when looking at base model performance 2 Epochs seems to work fine, after that we get some overfitting","metadata":{}},{"cell_type":"code","source":"# Function to build the model dynamically\n\"\"\"\n------\nSearching best parameters\n------\n\"\"\"\n\ndef build_model(hp):\n    model = Sequential([\n        Embedding(input_dim=10000, \n                  output_dim=hp.Choice('embedding_dim', [64, 128, 256]), \n                  input_length=MAX_LENGTH),\n        LSTM(hp.Choice('lstm_units', [32, 64, 128]), \n             return_sequences=False, \n             kernel_regularizer=l2(hp.Choice('l2_reg', [0.01, 0.05, 0.1]))),\n        Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)),\n        Dense(1, activation='sigmoid')\n    ])\n\n    optimizer = Adam(learning_rate=hp.Choice('learning_rate', [0.001, 0.0005, 0.00025]))\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n    return model\n\n# Initialize the tuner\ntuner = kt.Hyperband(\n    build_model,\n    objective='val_binary_accuracy',\n    max_epochs=10,\n    factor=3,\n    directory='my_tuning',\n    project_name='lstm_tuning'\n)\n\n# Start hyperparameter tuning\ntuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n\n# Get the best hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\n# Build the best model\nbest_model = tuner.hypermodel.build(best_hps)\n\n# Train the best model\nhistory2 = best_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=2,\n    batch_size=32\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model 3: with database without NaN**\n\nModel 2 is going to be trained taking into account more parameters and trainning with second database. ","metadata":{}},{"cell_type":"code","source":"#Model with best parameters - filtered database\n\"\"\"\n------\n Data preparation \n------\n\"\"\"\ndf_train_filtered = df_train.copy()  # Duplicate the dataset\ndf_train_filtered.dropna(inplace=True)  # Remove rows with null values\n\ntokenizer3 = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\ntokenizer3.fit_on_texts(df_train_filtered['clean_text'])\nsequences3 = tokenizer3.texts_to_sequences(df_train_filtered['clean_text'])\nMAX_LENGTH3 = max(len(seq) for seq in sequences3)\npadded_sequences3 = pad_sequences(sequences3, maxlen=MAX_LENGTH, padding='post', truncating='post')\nlabels3 = np.array(df_train_filtered['target'])\nX_train3, X_val3, y_train3, y_val3 = train_test_split(padded_sequences3, labels3, test_size=0.2, random_state=42)\n\"\"\"\n------\nModel Build and Train\n------\n\"\"\"\n#Model with filtered data\nhistory3 = best_model.fit(\n    X_train3, y_train3,\n    validation_data=(X_val3, y_val3),\n    epochs=2,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model 4: with all variables**\n\nWe have some data like keywords and location. let's include them in the model.\nFor this we are going to need to encode keyword and location. one-hot encode is okay here because we don`t nedd to capture relationships on various words\nthen we need to merge info from LSTM and categorical information and see if the classifier gets better with more info.","metadata":{}},{"cell_type":"code","source":"#Model with all variables - Filtered DB\n\"\"\"\n------\nData preparation \n------\n\"\"\"\n# One-hot encode `keyword` and `location`\nencoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nstructured_features = encoder.fit_transform(df_train_filtered[['keyword', 'location']])\n\n# Convert to numpy\nstructured_features = np.array(structured_features)\n\ntokenizer4 = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\ntokenizer4.fit_on_texts(df_train_filtered['clean_text'])\nsequences4 = tokenizer4.texts_to_sequences(df_train_filtered['clean_text'])\nMAX_LENGTH4 = max(len(seq) for seq in sequences4)\npadded_sequences4 = pad_sequences(sequences4, maxlen=MAX_LENGTH, padding='post', truncating='post')\nlabels4 = np.array(df_train_filtered['target'])\nX_train4, X_val4, y_train4, y_val4 = train_test_split(padded_sequences4, labels4, test_size=0.2, random_state=42)\n\"\"\"\n------\nModel Build and Train\n------\n\"\"\"\n# Text input (LSTM part)\ntext_input = Input(shape=(MAX_LENGTH,), name=\"text_input\")\nembedding = Embedding(input_dim=10000, output_dim=128)(text_input)\nlstm = LSTM(64)(embedding)\n\n# Structured input (categorical features)\nstructured_input = Input(shape=(structured_features.shape[1],), name=\"structured_input\")\ndense_features = Dense(32, activation='relu')(structured_input)\n\n# Merge both inputs\nmerged = Concatenate()([lstm, dense_features])\noutput = Dense(1, activation='sigmoid')(merged)\n\n# Define the model\nmodel4 = Model(inputs=[text_input, structured_input], outputs=output)\n\n# Compile model\nmodel4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n\nhistory4 = model4.fit(\n    [padded_sequences4, structured_features],  # Input: [Text + Structured]\n    labels4,  # Target labels\n    validation_split=0.2,\n    epochs=2,\n    batch_size=32\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Very similar performance as base model, so i wont waste time tuning parameters here. les's move on\n\n**Model 5**\n\nFinally we are going to test a simple model using just keywords; using filtered data.","metadata":{}},{"cell_type":"code","source":"#Model with only keywords\n\"\"\"\n------\nData preparation \n------\n\"\"\"\ntokenizer5 = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\ntokenizer5.fit_on_texts(df_train_filtered['keyword'])\nsequences5 = tokenizer.texts_to_sequences(df_train_filtered['keyword'])\nMAX_LENGTH = max(len(seq) for seq in sequences5)\npadded_sequences5 = pad_sequences(sequences5, maxlen=MAX_LENGTH, padding='post', truncating='post')\nlabels5 = np.array(df_train_filtered['target'])\nX_train5, X_val5, y_train5, y_val5 = train_test_split(padded_sequences5, labels5, test_size=0.2, random_state=42)\n\n\"\"\"\n------\nModel Build and Train\n------\n\"\"\"\n\n#Model with filtered data\nhistory5 = best_model.fit(\n    X_train5, y_train5,\n    validation_data=(X_val5, y_val5),\n    epochs=2,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results\nAfter testing various models we will stay with the model 2 that was trained only with text from tweets and with best parameters","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract accuracy and loss from training history\nhistory_dict = history2.history\n\ntrain_acc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\ntrain_loss = history_dict['loss']\nval_loss = history_dict['val_loss']\nepochs = range(1, len(train_acc) + 1)\n\n# Plot Accuracy\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training & Validation Accuracy')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion & submission\n\nIn conclusion i did:\n1. Model 1 using text from tweets and relating it with class\n2. Model 2 is testing hyperparametes for model 1 and improving its performance **(this one is overall best)**\n4. Model 3 is Model 2 trained just with those cases where location and keyword was avalible\n5. Model 4 is A hybrid model with LSTM and info obtained from location or keyword. Because of this it was only trained with the database where location and keyword was avalible\n6. Model 5 is the simplest model just taking into account the info from kewyord. \n\nFirst 4 models perform very similarly (near to 0.8 on binary accuracy), all of this models start to overfit preatty fast so we did early stopped preatty fast.\nFor further improvement i suspect we net to better filter data and maybe test alternative embeddings and words tokens.","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntokenizer_final = Tokenizer ()\nfinal_test_sequences = tokenizer_final.texts_to_sequences(df_test['text'])  # Assuming df_test is your test DataFrame\npadded_test_sequences = pad_sequences(final_test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n#Predict probabilities\ny_pred_prob = best_model.predict(padded_test_sequences)\n# Convert probabilities to binary labels (threshold = 0.5)\ny_pred = (y_pred_prob >= 0.5).astype(int)\n\n# Convert to DataFrame\nsubmission = pd.DataFrame({'id': df_test['id'], 'target': y_pred.flatten()})\n\n# Save as CSV\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Predictions saved successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}